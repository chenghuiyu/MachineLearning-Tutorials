{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Republican and Democratic donations\n",
    "\n",
    "## Part 2\n",
    "\n",
    "\n",
    "下面以“预测共和党和民主党的捐款”为例，进行说明，数据下载[地址](https://www.dataquest.io/blog/large_files/input.csv)。\n",
    "\n",
    "在Part1中已经对多个模型进行了平均化处理，为了更好的区别模型之间的性能差别，下面采用平均加权的方法进行数据的处理，即在对每个模型赋予初始权值，再进行平均处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# set seed to reproducibility\n",
    "SEED = 222\n",
    "np.random.seed(SEED)\n",
    "\n",
    "#读取数据\n",
    "df = pd.read_csv(\"/tmp/data_input/kaggle/Predicting_donations/input.csv\")\n",
    "\n",
    "#处理数据\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def get_train_test(test_size = 0.95):\n",
    "    # 将数据分为训练集和测试集\n",
    "    # 获取共和党的标签\n",
    "    y = 1 * (df.cand_pty_affiliation == \"REP\")\n",
    "    #去掉除共和党和民主党其他党派的捐款\n",
    "    X = df.drop([\"cand_pty_affiliation\"], axis=1)\n",
    "    X = pd.get_dummies(X, sparse=True)\n",
    "    X.drop(X.columns[X.std() == 0], axis=1, inplace=True)\n",
    "    \n",
    "    return train_test_split(X, y, test_size=test_size)\n",
    "\n",
    "#将原始数据处理为训练数据集和测试数据集\n",
    "xtrain, xtest, ytrain, ytest = get_train_test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 集成更多的模型来对数据进行训练\n",
    "from sklearn.svm import SVC, LinearSVC  # 支持向量机\n",
    "from sklearn.naive_bayes import GaussianNB # 高斯朴素贝叶斯\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier # 随机森林和Boosting\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier # K近邻\n",
    "from sklearn.neural_network import MLPClassifier # 神经网络\n",
    "from sklearn.kernel_approximation import Nystroem, RBFSampler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "\n",
    "#生成基本的学习器\n",
    "def get_models():\n",
    "    nb = GaussianNB();\n",
    "    svc = SVC(C=100, probability=True)\n",
    "    knn = KNeighborsClassifier(n_neighbors=3)\n",
    "    lr = LogisticRegression(C=100, random_state=SEED)\n",
    "    nn = MLPClassifier((80, 10), early_stopping=False,  random_state=SEED)\n",
    "    gb = GradientBoostingClassifier(n_estimators=10, random_state=SEED)\n",
    "    rf = RandomForestClassifier(n_estimators=10, max_features=3, random_state=SEED)\n",
    "    \n",
    "    models = {\"svm\": svc,\n",
    "             \"knn\":knn,\n",
    "             \"native bayes\": nb,\n",
    "             \"lr\": lr,\n",
    "             \"nn\": nn,\n",
    "             \"boosting\": gb,\n",
    "             \"random forest\": rf,\n",
    "             }\n",
    "    \n",
    "    return models\n",
    "\n",
    "\n",
    "# 开始进行模型的训练\n",
    "def train_predict(models_list):\n",
    "    # fit多个学习模型，并返回预测结果\n",
    "    store = np.zeros((ytest.shape[0], len(models_list)))\n",
    "    store = pd.DataFrame(store)\n",
    "    \n",
    "    print(\"Starting to fit\\n\")\n",
    "    cols = list()\n",
    "    for i, (name, model) in enumerate(models_list.items()):\n",
    "        print(\"%s...\" % name, end=\" \", flush=False)\n",
    "        model.fit(xtrain, ytrain)\n",
    "        store.iloc[:, i] = model.predict_proba(xtest)[:, 1]\n",
    "        cols.append(name)\n",
    "        print(\"model done\\n\")\n",
    "    store.columns = cols\n",
    "    print(\"Done\\n\")\n",
    "    \n",
    "    return store\n",
    "\n",
    "\n",
    "# 构建预测模型\n",
    "def score_model(y, store):\n",
    "    print(\"Scoring model\\n\")\n",
    "    for m in store.columns:\n",
    "        score = roc_auc_score(y, store.loc[:, m])\n",
    "        print (\"%-26s: %.3f\" % (m, score))\n",
    "    print(\"Done\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**（一）定义基学习模型的库**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_learners = get_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**（二）定义元学习器**\n",
    "\n",
    "目前远学习器的选择有多种，包括logistics regression，SVM，KNN，Decision Tree等，也可以采用另外一个ensemble的集成学习器，下面采用Gradient Boosting Machine（GBM）。\n",
    "\n",
    "为了确保 GBM 能够探索局部特征，我们需要限定每 1000 个决策树在 4 个基学习器的随机子集和 50% 的输入数据上进行训练。这样，GBM 就会表达每个基学习器在不同近邻输入空间上的预测内容。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "meta_learner = GradientBoostingClassifier(n_estimators=1000,\n",
    "                                         loss=\"exponential\",\n",
    "                                         max_features = 4,\n",
    "                                         max_depth = 3,\n",
    "                                         subsample=0.5,\n",
    "                                         learning_rate=0.005,\n",
    "                                         random_state=SEED)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**（三）生成训练集合测试集**\n",
    "\n",
    "为基学习器准备训练集\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xtrain_base, xpre_base, ytrain_base, ypre_base = train_test_split(xtrain, \n",
    "                                                                  ytrain, \n",
    "                                                                  test_size=0.5, \n",
    "                                                                  random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**（四）在训练集上训练基学习器**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_base_learners(base_learner, inp, out, verbose=True):\n",
    "    \"\"\"train all base learners in the lib\"\"\"\n",
    "    if verbose:\n",
    "        print(\"Fitting models\")\n",
    "        \n",
    "    for i , (name, m) in enumerate(base_learner.items()):\n",
    "        if verbose:\n",
    "            print(\"%s ...\" % name, end=\" \", flush=False)\n",
    "            m.fit(inp, out)\n",
    "        if verbose:\n",
    "            print(\"Done\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting models\n",
      "svm ... Done\n",
      "\n",
      "knn ... Done\n",
      "\n",
      "native bayes ... Done\n",
      "\n",
      "lr ... Done\n",
      "\n",
      "nn ... Done\n",
      "\n",
      "boosting ... Done\n",
      "\n",
      "random forest ... Done\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 训练学习器\n",
    "train_base_learners(base_learners, xtrain_base, ytrain_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(五) 对数据进行预测**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_base_learners(pre_base_learners, inp, verbose=True):\n",
    "    \"\"\"generate a prediction matrix\"\"\"\n",
    "    P = np.zeros((inp.shape[0], len(pre_base_learners)))\n",
    "    if verbose:\n",
    "        print(\"Generate models predictions\\n\")\n",
    "    for i, (name, m) in enumerate(pre_base_learners.items()):\n",
    "        if verbose:\n",
    "            print(\"%s ... \" % name, end=\" \", flush=False)\n",
    "            pre = m.predict_proba(inp)\n",
    "            P[:, i] = pre[:,1]\n",
    "        if verbose:\n",
    "            print(\"Done\\n\")\n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate models predictions\n",
      "\n",
      "svm ...  Done\n",
      "\n",
      "knn ...  Done\n",
      "\n",
      "native bayes ...  Done\n",
      "\n",
      "lr ...  Done\n",
      "\n",
      "nn ...  Done\n",
      "\n",
      "boosting ...  Done\n",
      "\n",
      "random forest ...  Done\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#对学习器进行预测\n",
    "P_base = predict_base_learners(base_learners, xpre_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**（六）对元学习器进行训练**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.005, loss='exponential', max_depth=3,\n",
       "              max_features=4, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
       "              presort='auto', random_state=222, subsample=0.5, verbose=0,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_learner.fit(P_base, ypre_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ensemble_predict(base_learners, meta_learner, inp, verbose=True):\n",
    "    \"\"\"Generate prediction from ensemble\"\"\"\n",
    "    P_pred = predict_base_learners(base_learner,inp,verbose=verbose)\n",
    "    return P_pred, meta_learner.predict_proba(P_pred)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate models predictions\n",
      "\n",
      "svm ...  Done\n",
      "\n",
      "knn ...  Done\n",
      "\n",
      "native bayes ...  Done\n",
      "\n",
      "lr ...  Done\n",
      "\n",
      "nn ...  Done\n",
      "\n",
      "boosting ...  Done\n",
      "\n",
      "random forest ...  Done\n",
      "\n",
      "\n",
      "Ensemble ROC-AUC score: 0.880\n"
     ]
    }
   ],
   "source": [
    "P_prd, p = ensemble_predict(base_learners, meta_learner, xtest, verbose=True)\n",
    "print(\"\\nEnsemble ROC-AUC score: %.3f\" % roc_auc_score(ytest, p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上述的结果可以看出，这次集成学习模型优于之前的集成学习，但是仍然低于简单的决策树估计，这主要是只对一半的数据进行基学习器和元学习器的训练，所以大量的信息丢失了。为了防止这点，下面使用交叉验证策略进行进一步的优化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 交叉验证训练\n",
    "\n",
    "\n",
    "在交叉验证训练基学习器时，每个基学习器的备份都进行了 K-1 fold 的拟合，并进行了剩余 fold 的预测。这一过程不断重复，直到每个 fold 都被预测。我们指定的 fold 越多，每次训练过程中的数据就越少。这使得交叉验证的预测在测试期间噪声更小，性能更好。但这显著增加了训练时间。通过交叉验证拟合一个集成经常被称为堆叠（stacking），而集成本身会被称为超级学习器（Super Learner）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import clone\n",
    "\n",
    "def stacking(base_learners, meta_learner, X, y, generator):\n",
    "    \"\"\"Simple training routine for stacking.\"\"\"\n",
    "    print(\"Fitting final base learners...\", end=\" \")\n",
    "    train_base_learners(base_learners, X, y, verbose=False)\n",
    "    print(\"done\")\n",
    "    \n",
    "    # Generate predictions for training meta learners\n",
    "    # Outer loop:\n",
    "    \n",
    "    print(\"Generating cross-validated predictions...\")\n",
    "    cv_pre, cv_y = [],[]\n",
    "    \n",
    "    for i, (train_idx, test_idx) in enumerate(generator.split(X)):\n",
    "        \n",
    "        flod_xtrain, flod_ytrain = X[train_idx,:], y[train_idx]\n",
    "        flod_xtest, flod_ytest = X[test_idx,:], y[test_idx]\n",
    "        \n",
    "        flod_base_learners = {name: clone(learner) \n",
    "                              for name, learner in base_learners.items()}\n",
    "        train_base_learners(flod_base_learners, flod_xtrain, flod_ytrain, verbose=False)\n",
    "        flod_P_base = predict_base_learners(flod_base_learners, flod_xtest, verbose=False)\n",
    "        \n",
    "        cv_pre.append(flod_P_base)\n",
    "        cv_y.append(flod_ytest)\n",
    "        \n",
    "        print(\"Flod %i is done\" % (i + 1))\n",
    "    \n",
    "    print(\"CV-predictions done\")\n",
    "    \n",
    "    cv_pre = np.vstack(cv_pre)\n",
    "    cv_y = np.hstack(cv_y)\n",
    "    \n",
    "    print(\"Fitting meta learning...\", end=\" \")\n",
    "    meta_learner.fit(cv_pre, cv_y)\n",
    "    print(\"Done\\n\")\n",
    "    \n",
    "    return base_learners, meta_learner\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting final base learners... done\n",
      "Generating cross-validated predictions...\n",
      "Flod 1 is done\n",
      "Flod 2 is done\n",
      "CV-predictions done\n",
      "Fitting meta learning... Done\n",
      "\n",
      "\n",
      "Ensemble ROC-AUC score: 0.500\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "cv_base_learners, cv_meta_learner = stacking(get_models(), \n",
    "                                             clone(meta_learner), \n",
    "                                             xtrain.values, \n",
    "                                             ytrain.values, \n",
    "                                             KFold(2))\n",
    "\n",
    "P_pre, p = ensemble_predict(cv_base_learners, cv_meta_learner, xtest, verbose=False)\n",
    "print(\"\\nEnsemble ROC-AUC score: %.3f\" % roc_auc_score(ytest, p))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用工具包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fitting 2 layers\n",
      "Processing layer-1             done | 00:03:10\n",
      "Processing layer-2             done | 00:00:02\n",
      "Fit complete                        | 00:03:14\n",
      "\n",
      "Predicting 2 layers\n",
      "Processing layer-1             done | 00:00:52\n",
      "Processing layer-2             done | 00:00:01\n",
      "Predict complete                    | 00:00:54\n",
      "\n",
      "Super Learner ROC_AUC score: 0.888\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from mlens.ensemble import SuperLearner\n",
    "# Instantiate the ensemble with 10 folds\n",
    "sl = SuperLearner(folds=10, random_state=SEED, verbose=2, backend=\"multiprocessing\")\n",
    "\n",
    "# Add the base learners and the meta learner\n",
    "sl.add(list(base_learners.values()), proba=True)\n",
    "sl.add_meta(meta_learner, proba=True)\n",
    "\n",
    "# Train the ensemble\n",
    "\n",
    "sl.fit(xtrain, ytrain)\n",
    "\n",
    "# Predict the test set\n",
    "p_sl = sl.predict_proba(xtest)\n",
    "print(\"\\nSuper Learner ROC_AUC score: %.3f\" % roc_auc_score(ytest, p_sl[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
